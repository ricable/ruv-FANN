//! PFS-REG-01 Model Registry Client Example

use anyhow::{Context, Result};
use clap::Parser;
use pfs_reg_01::generated::{
    model_registry_client::ModelRegistryClient,
    *,
};
use std::collections::HashMap;
use tonic::transport::Channel;
use tracing::{info, error};

#[derive(Parser)]
#[command(name = "pfs-reg-01-client")]
#[command(about = "PFS-REG-01 Model Registry Client")]
struct Args {
    /// Server endpoint
    #[arg(short, long, default_value = "http://127.0.0.1:50052")]
    endpoint: String,
    
    /// Command to execute
    #[command(subcommand)]
    command: Command,
}

#[derive(clap::Subcommand)]
enum Command {
    /// Register a new model
    Register {
        /// Model name
        #[arg(short, long)]
        name: String,
        
        /// Model description
        #[arg(short, long)]
        description: Option<String>,
        
        /// Model category
        #[arg(short, long, default_value = "neural-network")]
        category: String,
        
        /// Model artifact file path
        #[arg(short, long)]
        artifact: std::path::PathBuf,
    },
    
    /// Get model information
    Get {
        /// Model ID
        model_id: String,
        
        /// Include artifact data
        #[arg(long)]
        include_artifact: bool,
    },
    
    /// List models
    List {
        /// Page size
        #[arg(long, default_value = "10")]
        page_size: i32,
        
        /// Page token
        #[arg(long)]
        page_token: Option<String>,
    },
    
    /// Search models
    Search {
        /// Search query
        query: String,
        
        /// Page size
        #[arg(long, default_value = "10")]
        page_size: i32,
    },
    
    /// Get registry statistics
    Stats,
}

#[tokio::main]
async fn main() -> Result<()> {
    let args = Args::parse();
    
    // Initialize tracing
    tracing_subscriber::fmt::init();
    
    info!("Connecting to Model Registry at: {}", args.endpoint);
    
    // Connect to the server
    let channel = Channel::from_shared(args.endpoint)
        .context("Invalid endpoint")?
        .connect()
        .await
        .context("Failed to connect to server")?;
    
    let mut client = ModelRegistryClient::new(channel);
    
    match args.command {
        Command::Register { name, description, category, artifact } => {
            register_model(&mut client, name, description, category, artifact).await?;
        }
        Command::Get { model_id, include_artifact } => {
            get_model(&mut client, model_id, include_artifact).await?;
        }
        Command::List { page_size, page_token } => {
            list_models(&mut client, page_size, page_token).await?;
        }
        Command::Search { query, page_size } => {
            search_models(&mut client, query, page_size).await?;
        }
        Command::Stats => {
            get_stats(&mut client).await?;
        }
    }
    
    Ok(())
}

async fn register_model(
    client: &mut ModelRegistryClient<Channel>,
    name: String,
    description: Option<String>,
    category: String,
    artifact_path: std::path::PathBuf,
) -> Result<()> {
    info!("Registering model: {}", name);
    
    // Read artifact file
    let artifact_data = tokio::fs::read(&artifact_path)
        .await
        .with_context(|| format!("Failed to read artifact file: {:?}", artifact_path))?;
    
    info!("Artifact size: {} bytes", artifact_data.len());
    
    let model_info = ModelInfo {
        model_id: String::new(), // Will be generated by server
        name: name.clone(),
        description: description.unwrap_or_default(),
        version: "1.0.0".to_string(),
        category: match category.as_str() {
            "predictive-optimization" => ModelCategory::PredictiveOptimization as i32,
            "service-assurance" => ModelCategory::ServiceAssurance as i32,
            "network-intelligence" => ModelCategory::NetworkIntelligence as i32,
            "forecasting" => ModelCategory::Forecasting as i32,
            "classification" => ModelCategory::Classification as i32,
            "regression" => ModelCategory::Regression as i32,
            _ => ModelCategory::Unspecified as i32,
        },
        model_type: ModelType::NeuralNetwork as i32,
        tags: vec!["test".to_string(), "example".to_string()],
        capabilities: Some(ModelCapabilities {
            supports_batch_prediction: true,
            supports_streaming_prediction: false,
            supports_online_learning: false,
            supports_gpu_acceleration: false,
            supports_quantization: false,
            supports_pruning: false,
            max_input_size: 100,
            expected_output_size: 1,
            supported_data_types: vec!["f32".to_string()],
        }),
        configuration: Some(ModelConfiguration {
            hyperparameters: HashMap::new(),
            training_config: "{}".to_string(),
            architecture_config: "{}".to_string(),
            feature_names: vec![],
            target_kpi: "accuracy".to_string(),
            training_data: None,
        }),
        metadata: Some(ModelMetadata {
            author: "Test User".to_string(),
            organization: "RAN Intelligence Platform".to_string(),
            license: "MIT".to_string(),
            documentation_url: "".to_string(),
            repository_url: "".to_string(),
            papers: vec![],
            custom_fields: HashMap::new(),
        }),
        created_at: None,
        updated_at: None,
        created_by: "client".to_string(),
        status: ModelStatus::Registered as i32,
    };
    
    let request = tonic::Request::new(RegisterModelRequest {
        model_info: Some(model_info),
        model_artifact: artifact_data,
        auto_deploy: false,
    });
    
    match client.register_model(request).await {
        Ok(response) => {
            let resp = response.into_inner();
            if resp.success {
                info!("Model registered successfully!");
                info!("Model ID: {}", resp.model_id);
                info!("Version ID: {}", resp.version_id);
            } else {
                error!("Failed to register model: {}", resp.message);
            }
        }
        Err(e) => {
            error!("gRPC error: {}", e);
        }
    }
    
    Ok(())
}

async fn get_model(
    client: &mut ModelRegistryClient<Channel>,
    model_id: String,
    include_artifact: bool,
) -> Result<()> {
    info!("Getting model: {}", model_id);
    
    let request = tonic::Request::new(GetModelRequest {
        model_id: model_id.clone(),
        version_id: String::new(), // Get latest version
        include_artifact,
    });
    
    match client.get_model(request).await {
        Ok(response) => {
            let resp = response.into_inner();
            if let Some(model_info) = resp.model_info {
                info!("Model found:");
                info!("  ID: {}", model_info.model_id);
                info!("  Name: {}", model_info.name);
                info!("  Description: {}", model_info.description);
                info!("  Category: {:?}", ModelCategory::from_i32(model_info.category));
                info!("  Created by: {}", model_info.created_by);
                
                if let Some(version) = resp.version_info {
                    info!("  Version: {}", version.version_number);
                    info!("  Version ID: {}", version.version_id);
                    info!("  Artifact size: {} bytes", version.artifact.map(|a| a.size_bytes).unwrap_or(0));
                }
                
                if include_artifact {
                    info!("  Artifact data size: {} bytes", resp.model_artifact.len());
                }
            } else {
                warn!("Model not found: {}", model_id);
            }
        }
        Err(e) => {
            error!("gRPC error: {}", e);
        }
    }
    
    Ok(())
}

async fn list_models(
    client: &mut ModelRegistryClient<Channel>,
    page_size: i32,
    page_token: Option<String>,
) -> Result<()> {
    info!("Listing models (page_size: {})", page_size);
    
    let request = tonic::Request::new(ListModelsRequest {
        category: ModelCategory::Unspecified as i32,
        status: ModelStatus::Unspecified as i32,
        page_size,
        page_token: page_token.unwrap_or_default(),
    });
    
    match client.list_models(request).await {
        Ok(response) => {
            let resp = response.into_inner();
            info!("Found {} models (total: {})", resp.models.len(), resp.total_count);
            
            for model in resp.models {
                info!("  Model: {} - {} ({})", model.model_id, model.name, model.description);
            }
            
            if !resp.next_page_token.is_empty() {
                info!("Next page token: {}", resp.next_page_token);
            }
        }
        Err(e) => {
            error!("gRPC error: {}", e);
        }
    }
    
    Ok(())
}

async fn search_models(
    client: &mut ModelRegistryClient<Channel>,
    query: String,
    page_size: i32,
) -> Result<()> {
    info!("Searching models: '{}'", query);
    
    let request = tonic::Request::new(SearchModelsRequest {
        query,
        tags: vec![],
        category: ModelCategory::Unspecified as i32,
        model_type: ModelType::Unspecified as i32,
        page_size,
        page_token: String::new(),
    });
    
    match client.search_models(request).await {
        Ok(response) => {
            let resp = response.into_inner();
            info!("Found {} models", resp.models.len());
            
            for model in resp.models {
                info!("  Model: {} - {} ({})", model.model_id, model.name, model.description);
            }
            
            if !resp.suggestions.is_empty() {
                info!("Suggestions: {:?}", resp.suggestions);
            }
        }
        Err(e) => {
            error!("gRPC error: {}", e);
        }
    }
    
    Ok(())
}

async fn get_stats(client: &mut ModelRegistryClient<Channel>) -> Result<()> {
    info!("Getting registry statistics");
    
    let request = tonic::Request::new(google::protobuf::Empty {});
    
    match client.get_registry_stats(request).await {
        Ok(response) => {
            let stats = response.into_inner();
            info!("Registry Statistics:");
            info!("  Total models: {}", stats.total_models);
            info!("  Total versions: {}", stats.total_versions);
            info!("  Active deployments: {}", stats.active_deployments);
            info!("  Total predictions served: {}", stats.total_predictions_served);
            info!("  Average latency: {:.2}ms", stats.average_model_latency_ms);
            
            if !stats.models_by_category.is_empty() {
                info!("  Models by category:");
                for (category, count) in stats.models_by_category {
                    info!("    {}: {}", category, count);
                }
            }
            
            if !stats.models_by_status.is_empty() {
                info!("  Models by status:");
                for (status, count) in stats.models_by_status {
                    info!("    {}: {}", status, count);
                }
            }
        }
        Err(e) => {
            error!("gRPC error: {}", e);
        }
    }
    
    Ok(())
}